{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "AXwmgkZgLpqg",
        "outputId": "36045a07-8278-4cc1-8618-614051578860"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e5974a09f783>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#model_path = '../models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0D.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdata0D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/fraunhofer_eas_dataset_for_unbalance_detection_v1.zip'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Option a) local file contains a small subset of the entire dataset\n",
        "url = \"D:/fraunhofer_eas_dataset_for_unbalance_detection_v1.zip\"\n",
        "\n",
        "# Option b) the entire dataset can be directly downloaded via public Fraunhofer Fortadis dataspace\n",
        "#url = 'https://fordatis.fraunhofer.de/bitstream/fordatis/151.2/1/fraunhofer_eas_dataset_for_unbalance_detection_v1.zip'\n",
        "\n",
        "# Option c) selected pre-trained models can be found in the directory model/reference\n",
        "use_reference_models = True\n",
        "model_path = '../models/reference'\n",
        "\n",
        "# Option d) all models will be trained again\n",
        "#use_reference_models = False\n",
        "#model_path = '../models'\n",
        "\n",
        "with zipfile.ZipFile(url, 'r') as f:\n",
        "    with f.open('0D.csv', 'r') as c:\n",
        "        data0D = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('0E.csv', 'r') as c:\n",
        "        data0E = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('1D.csv', 'r') as c:\n",
        "        data1D = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('1E.csv', 'r') as c:\n",
        "        data1E = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('2D.csv', 'r') as c:\n",
        "        data2D = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('2E.csv', 'r') as c:\n",
        "        data2E = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('3D.csv', 'r') as c:\n",
        "        data3D = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('3E.csv', 'r') as c:\n",
        "        data3E = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('4D.csv', 'r') as c:\n",
        "        data4D = pd.read_csv(c)\n",
        "        print('done')\n",
        "    with f.open('4E.csv', 'r') as c:\n",
        "        data4E = pd.read_csv(c)\n",
        "        print('done')\n",
        "\n",
        "        labels = {'no_unbalance':0, 'unbalance':1}\n",
        "sensor = 'Vibration_1'\n",
        "samples_per_second = 4096\n",
        "seconds_per_analysis = 1.0\n",
        "window = int(samples_per_second*seconds_per_analysis)\n",
        "\n",
        "def get_features(data, label):\n",
        "    n = int(np.floor(len(data)/window))\n",
        "    data = data[:int(n)*window]\n",
        "    X = data.values.reshape((n, window))\n",
        "    y = np.ones(n)*labels[label]\n",
        "    return X,y\n",
        "\n",
        "X0,y0 = get_features(data0D[sensor], \"no_unbalance\")\n",
        "X1,y1 = get_features(data1D[sensor], \"unbalance\")\n",
        "X2,y2 = get_features(data2D[sensor], \"unbalance\")\n",
        "X3,y3 = get_features(data3D[sensor], \"unbalance\")\n",
        "X4,y4 = get_features(data4D[sensor], \"unbalance\")\n",
        "X=np.concatenate([X0, X1, X2, X3, X4])\n",
        "y=np.concatenate([y0, y1, y2, y3, y4])\n",
        "\n",
        "X0_val, y0_val = get_features(data0E[sensor], \"no_unbalance\")\n",
        "X1_val, y1_val = get_features(data1E[sensor], \"unbalance\")\n",
        "X2_val, y2_val = get_features(data2E[sensor], \"unbalance\")\n",
        "X3_val, y3_val = get_features(data3E[sensor], \"unbalance\")\n",
        "X4_val, y4_val = get_features(data4E[sensor], \"unbalance\")\n",
        "X_val=np.concatenate([X0_val, X1_val, X2_val, X3_val, X4_val])\n",
        "\n",
        "print(X.shape, y.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_test_ratio = 0.9\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 1-train_test_ratio, random_state = 0)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "X_fft = np.abs(np.fft.rfft(X, axis=1))[:,:int(window/2)]\n",
        "X_train_fft = np.abs(np.fft.rfft(X_train, axis=1))[:,:int(window/2)]\n",
        "X_test_fft = np.abs(np.fft.rfft(X_test, axis=1))[:,:int(window/2)]\n",
        "X_val_fft = np.abs(np.fft.rfft(X_val, axis=1))[:,:int(window/2)]\n",
        "\n",
        "X_fft[:,0]=0\n",
        "X_train_fft[:,0]=0\n",
        "X_test_fft[:,0]=0\n",
        "X_val_fft[:,0]=0\n",
        "\n",
        "print(X_train_fft.shape, X_test_fft.shape, X_val_fft.shape)\n",
        "scaler = RobustScaler(quantile_range=(5,95)).fit(X_train_fft)\n",
        "\n",
        "X_fft_sc = scaler.transform(X_fft)\n",
        "X_train_fft_sc = scaler.transform(X_train_fft)\n",
        "X_test_fft_sc = scaler.transform(X_test_fft)\n",
        "X_val_fft_sc = scaler.transform(X_val_fft)\n",
        "\n",
        "X_val_fft_1 = X_val_fft_sc[:len(y0_val),:]\n",
        "y_val_1 = y_val[:len(y0_val)]\n",
        "X_val_fft_2 = X_val_fft_sc[len(y0_val):len(y0_val)+len(y1_val),:]\n",
        "y_val_2 = y_val[len(y0_val):len(y0_val)+len(y1_val)]\n",
        "X_val_fft_3 = X_val_fft_sc[len(y0_val)+len(y1_val):len(y0_val)+\n",
        "                           len(y1_val)+len(y2_val),:]\n",
        "y_val_3 = y_val[len(y0_val)+len(y1_val):len(y0_val)+len(y1_val)+\n",
        "                len(y2_val)]\n",
        "X_val_fft_4 = X_val_fft_sc[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+\n",
        "                           len(y1_val)+len(y2_val)+len(y3_val),:]\n",
        "y_val_4 = y_val[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+len(y1_val)+\n",
        "                len(y2_val)+len(y3_val)]\n",
        "X_val_fft_5 = X_val_fft_sc[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+\n",
        "                           len(y1_val)+len(y2_val)+len(y3_val)+len(y4_val),:]\n",
        "y_val_5 = y_val[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+len(y1_val)+\n",
        "                len(y2_val)+len(y3_val)+len(y4_val)]\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.layers import BatchNormalization,LeakyReLU,Dense,Dropout\n",
        "from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Flatten,ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "if not use_reference_models:\n",
        "\n",
        "    weight_for_0 = len(y)/(2*len(y[y==0]))\n",
        "    weight_for_1 = len(y)/(2*len(y[y==1]))\n",
        "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "    epochs = 100\n",
        "\n",
        "    for i in range(5):\n",
        "        X_in = Input(shape=(X_train_fft.shape[1],), name=\"cam_layer\")\n",
        "        x = X_in\n",
        "        for j in range(i):\n",
        "            x = Dense(units = 1024, activation=\"linear\")(x)\n",
        "            x = LeakyReLU(alpha=0.05)(x)\n",
        "        X_out = Dense(units = 1, activation = 'sigmoid')(x)\n",
        "        model_i = Model(X_in, X_out)\n",
        "\n",
        "\n",
        "        best_model_filepath = f\"{model_path}/fft_fcn_{i}_layers.h5\"\n",
        "        checkpoint = ModelCheckpoint(best_model_filepath, monitor='val_loss',\n",
        "                                     verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "        model_i.compile(optimizer = Adam(lr=0.0005), loss = 'binary_crossentropy',\n",
        "                        metrics = ['accuracy'])\n",
        "        model_i.summary()\n",
        "\n",
        "        model_i.fit(X_train_fft_sc, y_train, epochs = 100, batch_size = 128,\n",
        "                       validation_data=(X_test_fft_sc, y_test), callbacks=[checkpoint],\n",
        "                    class_weight=class_weight)\n",
        "\n",
        "        from tensorflow.keras.models import load_model\n",
        "\n",
        "model_loss = []\n",
        "model_acc = []\n",
        "model_accs_per_class = []\n",
        "\n",
        "for i in range(5):\n",
        "    best_model_filepath = f\"{model_path}/fft_fcn_{i}_layers.h5\"\n",
        "    model_i = load_model(best_model_filepath)\n",
        "    train_acc_ges = model_i.evaluate(X_train_fft_sc, y_train)\n",
        "    val_acc_ges = model_i.evaluate(X_val_fft_sc, y_val)\n",
        "\n",
        "    val_acc_1 = model_i.evaluate(X_val_fft_1, y_val_1)\n",
        "    val_acc_2 = model_i.evaluate(X_val_fft_2, y_val_2)\n",
        "    val_acc_3 = model_i.evaluate(X_val_fft_3, y_val_3)\n",
        "    val_acc_4 = model_i.evaluate(X_val_fft_4, y_val_4)\n",
        "    val_acc_5 = model_i.evaluate(X_val_fft_5, y_val_5)\n",
        "\n",
        "    model_acc.append([train_acc_ges[1], val_acc_ges[1]])\n",
        "    model_loss.append([train_acc_ges[0], val_acc_ges[0]])\n",
        "    model_accs_per_class.append([val_acc_1, val_acc_2, val_acc_3, val_acc_4, val_acc_5])\n",
        "\n",
        "\n",
        "fig=plt.figure(figsize=(12,8))\n",
        "ax1=plt.subplot(111)\n",
        "ax1.plot(np.array(model_loss)[:,1], color=\"tab:red\", marker=\"o\")\n",
        "ax1.tick_params('y', colors='tab:red')\n",
        "ax1.set_ylabel(ylabel = \"validation loss\", color=\"tab:red\")\n",
        "ax1.set_xticks([0,1,2,3,4])\n",
        "ax1.set_xlabel(\"N hidden layers\")\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(np.array(model_acc)[:,1], color=\"tab:blue\", marker=\"o\")\n",
        "ax2.tick_params('y', colors='tab:blue')\n",
        "ax2.set_ylabel(ylabel = \"validation accuracy\", color=\"tab:blue\")\n",
        "plt.show()\n",
        "\n",
        "np.array(model_accs_per_class)[0,:,1]\n",
        "\n",
        "np.array(model_accs_per_class)\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "model0 = load_model(f\"{model_path}/fft_fcn_0_layers.h5\")\n",
        "model1 = load_model(f\"{model_path}/fft_fcn_1_layers.h5\")\n",
        "model2 = load_model(f\"{model_path}/fft_fcn_2_layers.h5\")\n",
        "model3 = load_model(f\"{model_path}/fft_fcn_3_layers.h5\")\n",
        "model4 = load_model(f\"{model_path}/fft_fcn_4_layers.h5\")\n",
        "\n",
        "model2.summary()\n",
        "accuracies_per_class = []\n",
        "for i in range(5):\n",
        "    filepath_i = f\"{model_path}/fft_fcn_{i}_layers.h5\"\n",
        "    model_i = load_model(filepath_i)\n",
        "\n",
        "    val_acc_1 = model_i.evaluate(X_val_fft_1, y_val_1)\n",
        "    val_acc_2 = model_i.evaluate(X_val_fft_2, y_val_2)\n",
        "    val_acc_3 = model_i.evaluate(X_val_fft_3, y_val_3)\n",
        "    val_acc_4 = model_i.evaluate(X_val_fft_4, y_val_4)\n",
        "    val_acc_5 = model_i.evaluate(X_val_fft_5, y_val_5)\n",
        "    accuracies_per_class.append([val_acc_1, val_acc_2, val_acc_3, val_acc_4, val_acc_5])\n",
        "\n",
        "    fig=plt.figure(figsize=(12,8))\n",
        "ax1=plt.subplot(111, title = \"Unbalance Classification Trained with all Unbalances\")\n",
        "unbalances = np.array([0, 4.59e-5, 6.07e-5,7.55e-5,1.521e-4])\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[0,:,1], label=f\"0 hidden layers, mean: {100.0*np.mean(np.array(accuracies_per_class)[0,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[1,:,1], label=f\"1 hidden layer, mean: {100.0*np.mean(np.array(accuracies_per_class)[1,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[2,:,1], label=f\"2 hidden layers, mean: {100.0*np.mean(np.array(accuracies_per_class)[2,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[3,:,1], label=f\"3 hidden layers, mean: {100.0*np.mean(np.array(accuracies_per_class)[3,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[4,:,1], label=f\"4 hidden layers, mean: {100.0*np.mean(np.array(accuracies_per_class)[4,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Unbalance Factor [mm g]\")\n",
        "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from scipy.stats import mode\n",
        "\n",
        "def v2rpm(v):\n",
        "    return 212*v + 209\n",
        "\n",
        "# 3s ramp up\n",
        "fade_in = np.arange(0.0, 4.0, 4.0/(3*4096))\n",
        "# complete voltage sweep\n",
        "measurement_circle = np.repeat(np.arange(4.0, 8.2, 0.1), 4096*20.0)\n",
        "# measurement: start-up + 2 voltage sweeps\n",
        "measurement = np.concatenate([fade_in, np.tile(measurement_circle,3)])\n",
        "# select the data as actually used\n",
        "measurement1 = measurement[50000:]\n",
        "measurement1 = measurement1[:int(len(measurement1)/4096)*4096].reshape(-1,4096)\n",
        "voltages_measurement = mode(measurement1, axis=1)[0]\n",
        "voltages_used = np.concatenate([voltages_measurement[:len(X_val_fft_1)],\n",
        "                                voltages_measurement[:len(X_val_fft_2)],\n",
        "                                voltages_measurement[:len(X_val_fft_3)],\n",
        "                                voltages_measurement[:len(X_val_fft_4)],\n",
        "                                voltages_measurement[:len(X_val_fft_5)]])\n",
        "rpms_used = v2rpm(voltages_used)\n",
        "\n",
        "rpm_borders = np.arange(1050, 1975, 25)\n",
        "errors_per_rpm_range0 = []\n",
        "errors_per_rpm_range1 = []\n",
        "errors_per_rpm_range2 = []\n",
        "errors_per_rpm_range3 = []\n",
        "errors_per_rpm_range4 = []\n",
        "for i in range(len(rpm_borders)-1):\n",
        "    eval_inds = np.where((rpms_used>rpm_borders[i])&(rpms_used<rpm_borders[i+1]))[0]\n",
        "    errors_per_rpm_range0.append(\n",
        "        1-np.mean(np.abs(np.int32(model0.predict(X_val_fft_sc[eval_inds])>0.5).reshape(-1)-y_val[eval_inds])))\n",
        "    errors_per_rpm_range1.append(\n",
        "        1-np.mean(np.abs(np.int32(model1.predict(X_val_fft_sc[eval_inds])>0.5).reshape(-1)-y_val[eval_inds])))\n",
        "    errors_per_rpm_range2.append(\n",
        "        1-np.mean(np.abs(np.int32(model2.predict(X_val_fft_sc[eval_inds])>0.5).reshape(-1)-y_val[eval_inds])))\n",
        "    errors_per_rpm_range3.append(\n",
        "        1-np.mean(np.abs(np.int32(model3.predict(X_val_fft_sc[eval_inds])>0.5).reshape(-1)-y_val[eval_inds])))\n",
        "    errors_per_rpm_range4.append(\n",
        "        1-np.mean(np.abs(np.int32(model4.predict(X_val_fft_sc[eval_inds])>0.5).reshape(-1)-y_val[eval_inds])))\n",
        "fig=plt.figure(figsize=(12,8))\n",
        "plt.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range0, marker=\"+\", ls=\"--\", label=\"0 hidden layers\")\n",
        "plt.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range1, marker=\"+\", ls=\"--\", label=\"1 hidden layer\")\n",
        "plt.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range2, marker=\"+\", ls=\"--\", label=\"2 hidden layers\")\n",
        "plt.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range3, marker=\"+\", ls=\"--\", label=\"3 hidden layers\")\n",
        "plt.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range4, marker=\"+\", ls=\"--\", label=\"4 hidden layers\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Rotation Speed [rpm]\")\n",
        "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "X_dev = [X0, X1, X2, X3, X4]\n",
        "y_dev = [y0, y1, y2, y3, y4]\n",
        "\n",
        "X_val_separated = [X_val_fft_1, X_val_fft_2, X_val_fft_3, X_val_fft_4, X_val_fft_5]\n",
        "y_val_separated = [y_val_1, y_val_2, y_val_3, y_val_4, y_val_5]\n",
        "\n",
        "for layer_n in range(5):\n",
        "    for dataset_i in range(4):\n",
        "        X_dev_i = np.concatenate([X_dev[0], X_dev[dataset_i+1]])\n",
        "        y_dev_i = np.concatenate([y_dev[0], y_dev[dataset_i+1]])\n",
        "        X_val_i = np.concatenate([X_val_separated[0], X_val_separated[dataset_i+1]])\n",
        "        y_val_i = np.concatenate([y_val_separated[0], y_val_separated[dataset_i+1]])\n",
        "\n",
        "        train_test_ratio = 0.9\n",
        "        X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
        "            X_dev_i,y_dev_i, test_size = 1-train_test_ratio, random_state = 0)\n",
        "\n",
        "        X_train_fft_i = np.abs(np.fft.rfft(X_train_i, axis=1))[:,:int(window/2)]\n",
        "        X_test_fft_i = np.abs(np.fft.rfft(X_test_i, axis=1))[:,:int(window/2)]\n",
        "        X_train_fft_i[:,0]=0\n",
        "        X_test_fft_i[:,0]=0\n",
        "\n",
        "        scaler = RobustScaler(quantile_range=(5,95)).fit(X_train_fft_i)\n",
        "        X_train_fft_sc_i = scaler.transform(X_train_fft_i)\n",
        "        X_test_fft_sc_i = scaler.transform(X_test_fft_i)\n",
        "\n",
        "        if not use_reference_models:\n",
        "            weight_for_0 = len(y_dev_i)/(2*len(y_dev_i[y_dev_i==0]))\n",
        "            weight_for_1 = len(y_dev_i)/(2*len(y_dev_i[y_dev_i==1]))\n",
        "            class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "            X_in = Input(shape=(X_train_fft_i.shape[1],), name=\"cam_layer\")\n",
        "            previous_layer = X_in\n",
        "            for j in range(layer_n):\n",
        "                x = Dense(units = 1024, activation=\"linear\")(previous_layer)\n",
        "                x = LeakyReLU(alpha=0.05)(x)\n",
        "                previous_layer = x\n",
        "            X_out = Dense(units = 1, activation = 'sigmoid')(previous_layer)\n",
        "            model_i = Model(X_in, X_out)\n",
        "\n",
        "            best_model_filepath = f\"{model_path}_fft_fcn_{layer_n}_layers_dataset_pair_{dataset_i}.h5\"\n",
        "            checkpoint = ModelCheckpoint(best_model_filepath, monitor='val_loss',\n",
        "                                         verbose=1, save_best_only=True, mode='min')\n",
        "            model_i.compile(optimizer = Adam(lr=0.0005), loss = 'binary_crossentropy',\n",
        "                            metrics = ['accuracy'])\n",
        "            model_i.summary()\n",
        "\n",
        "            model_i.fit(X_train_fft_sc_i, y_train_i, epochs = 100, batch_size = 128,\n",
        "                       validation_data=(X_test_fft_sc_i, y_test_i), callbacks=[checkpoint],\n",
        "                        class_weight=class_weight)\n",
        "\n",
        "        best_model_filepath = f\"{model_path}/fft_fcn_{layer_n}_layers_dataset_pair_{dataset_i}.h5\"\n",
        "        model_i = load_model(best_model_filepath)\n",
        "        train_acc = model_i.evaluate(X_train_fft_sc_i, y_train_i)\n",
        "        val_acc = model_i.evaluate(X_val_i, y_val_i)\n",
        "        print(f\"Layers: {layer_n}, dataset pair {dataset_i}\")\n",
        "        print(train_acc)\n",
        "        print(val_acc)\n",
        "\n",
        "        accuracies = []\n",
        "for layer_n in range(5):\n",
        "    accuracies_layer_i = []\n",
        "    for dataset_i in range(4):\n",
        "        X_val_i = np.concatenate([X_val_separated[0], X_val_separated[dataset_i+1]])\n",
        "        y_val_i = np.concatenate([y_val_separated[0], y_val_separated[dataset_i+1]])\n",
        "\n",
        "        filepath = f\"{model_path}/fft_fcn_{layer_n}_layers_dataset_pair_{dataset_i}.h5\"\n",
        "        model_i = load_model(filepath)\n",
        "        accuracies_layer_i.append(model_i.evaluate(X_val_i, y_val_i)[1])\n",
        "    accuracies.append(accuracies_layer_i)\n",
        "accuracies = np.array(accuracies)\n",
        "accuracies_mean = 100*np.mean(accuracies, axis=1)\n",
        "fig=plt.figure(figsize=(12,8))\n",
        "ax1=plt.subplot(111, title = \"Pairwise Unbalance Classification\")\n",
        "unbalances = np.array([0, 4.59e-5, 6.07e-5,7.55e-5,1.521e-4])\n",
        "ax1.plot(1e6*unbalances[1:], accuracies[0,:], label=f\"0 hidden  FClayers, mean: {accuracies_mean[0]:.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances[1:], accuracies[1,:], label=f\"1 hidden  FClayer, mean: {accuracies_mean[1]:.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances[1:], accuracies[2,:], label=f\"2 hidden  FClayers, mean: {accuracies_mean[2]:.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances[1:], accuracies[3,:], label=f\"3 hidden  FClayers, mean: {accuracies_mean[3]:.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances[1:], accuracies[4,:], label=f\"4 hidden  FClayers, mean: {accuracies_mean[4]:.1f}%\", marker=\"+\", ls=\"--\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Unbalance Factor [mm g]\")\n",
        "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig=plt.figure(figsize=(15,5))\n",
        "ax1=plt.subplot(132, title = \"Unbalance Classification Trained\\nwith all Unbalances\")\n",
        "unbalances = np.array([0, 4.59e-5, 6.07e-5,7.55e-5,1.521e-4])\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[0,:,1],\n",
        "         label=f\"0 hidden FC layers, mean: {100*np.mean(np.array(accuracies_per_class)[0,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[1,:,1],\n",
        "         label=f\"1 hidden FC layer, mean: {100*np.mean(np.array(accuracies_per_class)[1,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[2,:,1],\n",
        "         label=f\"2 hidden FC layers, mean: {100*np.mean(np.array(accuracies_per_class)[2,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[3,:,1],\n",
        "         label=f\"3 hidden FC layers, mean: {100*np.mean(np.array(accuracies_per_class)[3,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax1.plot(1e6*unbalances, np.array(accuracies_per_class)[4,:,1],\n",
        "         label=f\"4 hidden FC layers, mean: {100*np.mean(np.array(accuracies_per_class)[4,:,1]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Unbalance Factor [mm g]\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.legend(loc=\"lower right\")\n",
        "ax1.text(-20, 1.05,\"(e)\", fontsize=12)\n",
        "ax2=plt.subplot(131, title = \"Pairwise Unbalance Classification\")\n",
        "ax2.plot(1e6*unbalances[1:], accuracies[0,:],\n",
        "         label=f\"0 hidden FC layers, mean: {100*np.mean(np.array(accuracies)[0,:]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax2.plot(1e6*unbalances[1:], accuracies[1,:],\n",
        "         label=f\"1 hidden FC layer, mean: {100*np.mean(np.array(accuracies)[1,:]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax2.plot(1e6*unbalances[1:], accuracies[2,:],\n",
        "         label=f\"2 hidden FC layers, mean: {100*np.mean(np.array(accuracies)[2,:]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax2.plot(1e6*unbalances[1:], accuracies[3,:],\n",
        "         label=f\"3 hidden FC layers, mean: {100*np.mean(np.array(accuracies)[3,:]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "ax2.plot(1e6*unbalances[1:], accuracies[4,:],\n",
        "         label=f\"4 hidden FC layers, mean: {100*np.mean(np.array(accuracies)[4,:]):.1f}%\", marker=\"+\", ls=\"--\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Unbalance Factor [mm g]\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.legend()\n",
        "ax2.text(33, 1.05,\"(d)\", fontsize=12)\n",
        "title = \"Unbalance Classification Trained with all\\nUnbalances: Rotation Speed Dependency\"\n",
        "ax3 = plt.subplot(133, title=title)\n",
        "ax3.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range0,\n",
        "         marker=\"+\", ls=\"--\", label=f\"0 hidden FC layers, mean: {100*np.mean(errors_per_rpm_range0):.1f}%\")\n",
        "ax3.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range1,\n",
        "         marker=\"+\", ls=\"--\", label=f\"1 hidden FC layer, mean: {100*np.mean(errors_per_rpm_range1):.1f}%\")\n",
        "ax3.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range2,\n",
        "         marker=\"+\", ls=\"--\", label=f\"2 hidden FC layers, mean: {100*np.mean(errors_per_rpm_range2):.1f}%\")\n",
        "ax3.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range3,\n",
        "         marker=\"+\", ls=\"--\", label=f\"3 hidden FC layers, mean: {100*np.mean(errors_per_rpm_range3):.1f}%\")\n",
        "ax3.plot(np.array(rpm_borders[:-1])+25, errors_per_rpm_range4,\n",
        "         marker=\"+\", ls=\"--\", label=f\"4 hidden FC layers, mean: {100*np.mean(errors_per_rpm_range4):.1f}%\")\n",
        "plt.ylabel(\"Accuracy on Evaluation Dataset\")\n",
        "plt.xlabel(\"Rotation Speed [RPM]\")\n",
        "plt.ylim([0.45, 1.05])\n",
        "plt.legend(loc=\"lower right\")\n",
        "ax3.text(960, 1.05,\"(f)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}